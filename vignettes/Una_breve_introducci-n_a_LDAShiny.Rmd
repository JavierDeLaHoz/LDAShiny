---
title: "Una breve introducción a LDAShiny"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Una breve introducción a LDAShiny}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


<center>
!["LDAShiny"](../inst/img/LOGO1.png) 
</center>

# Introduction

`LDAShiny` es una aplicación web shiny (Chang *et al.* 2017) fácil de usar  para realizar revisiónes exploratoria de literatura científica, que implementa el modelo probabilístico Latent Dirichlet Allocation (LDA) (Blei, Ng y Jordan 2003). La motivación para crear LDAShiny fue optimizar el flujo de trabajo de rutina de LDA para que los usuarios que no estén familiarizados con R puedan realizar el análisis de forma interactiva en un navegador web.


## Instalar y ejecutar LDAshiny

Para instalar la versión estable de la  CRAN 

```{r, eval=FALSE}
install.packages("LDAShiny")
```



Para iniciar `LDAShiny`
```{r, eval = FALSE}
library(LDAShiny)
LDAShiny::runLDAShiny()
```

Una vez lanzada la GUI se observaun menú que, de arriba a abajo, guía al usuario a través del análisis (Video 1) 


<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/m-BB-7Yh-NA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<center> Video 1 </center>



# Ejemplo de aplicación
 

## Adquisición de datos

Los datos se pueden obtener consultando la base de datos SCOPUS o Clarivate Analytics Web of Science (WoS) por diversos campos, como tema, autor, revista, período de tiempo, etc.

Para la demostración se realizó una revisión exploratoria de la literatura de la especie *Oreochromis niloticus*. Teniendo en cuenta los documentos en los que se mencionó el nombre de la especie en el título, resumen o palabra clave, asegurándose de que se incluyan tantos documentos potencialmente relevantes como sea posible (Figura 2)


```{r fig2,fig.cap='Figura 2. Busqueda en Scopus ',fig.align='center', echo=FALSE, out.width = '90%'}
knitr::include_graphics("../inst/img/Fig2.jpg")
```

Se encontraron 6196 resúmenes de artículos (en las últimas tres décadas de 1991 a junio de 2020). Esta cantidad de documentos hace que una revisión exploratoria individual lleve demasiado tiempo, por lo que el conjunto de artículos considerados proporciona un buen caso para probar la aplicación.

Usted puede descargar las referencias (hasta 2000 registros completos) marcando la casilla 'Seleccionar todo' y haciendo clic en el enlace 'Exportar'. Elija el tipo de archivo "Exportación CSV" y "Título del documento", "Año" y "Resumen" (Figura 3).


```{r fig3,fig.cap='Figura 3. Pasos para exportar',fig.align='center', echo=FALSE, out.width = '90%'}
knitr::include_graphics("../inst/img/Fig3.jpg")
```

La herramienta de exportación SCOPUS crea un archivo de exportación con el nombre predeterminado "scopus.csv". Puede descargar el archivo (`O.niloticus.csv`) que contiene los datos de este ejemplo [aquí](https://github.com/JavierDeLaHoz/o.niloticus/blob/main/O.niloticus.csv) . Para los archivos CSV, los caracteres acentuados no se muestran correctamente en Excel a menos que guarde el archivo y luego use la función de importación de datos de Excel para ver el archivo. También puede utilizar las siguientes instrucciones de R para descargar el archivo y guardarlo.


```{r,eval=FALSE}
library (readr)
urlfile="https://raw.githubusercontent.com/JavierDeLaHoz/o.niloticus/main/O.niloticus.csv"
O.niloticus <-read_csv(url(urlfile))
O.niloticus <- write.csv(O.niloticus,file="O.niloticus.csv")
```



## Preprocesamiento

El preprocesamiento busca normalizar o convertir el conjunto de texto a una forma estándar más conveniente, también permite la reducción de la dimensionalidad de la matriz de datos al eliminar el ruido o los términos sin sentido. El conjunto de datos requerido debe estar en un formato amplio (un artículo o resumen por fila). Sube elO.niloticus.csvarchivo de datos a LDAShiny desde el panel Cargar datos . A continuación, en el panel Limpieza de datos, haga clic en el botón `Incorporate information`, luego especifique las columnas para `id document` (Título en nuestro caso), `Select document vector` (Resumen) y Seleccione `Select publish year` (Año), posteriormente haga clic en la casilla de verificación para seleccionar `ngram` (Bigrams para esta demostración), eliminar números, seleccionar el idioma para las stopword e incluya otras palabras que desea eliminar, en nuestro caso usamos además de la lista predeterminada, una lista precompilada llamada SMART (System for the Mechanical Analysis and Retrieval of Text) del paquete stopword. La lista completa de palabras utilizadas para la opción de eliminación de stopword (palabras vacías) se pueden descargar [aquí](https://github.com/JavierDeLaHoz/stopword/blob/main/stopword.csv). Para este ejemplo no se realizó Stemming y en el slider de `Sparcity` se coloco en 0.995 (99.5%), es decir, los términos que aparecen en más del 0,5% de los documentos. Finalmente, se hace clic en el botón `Create DTM `(Matriz de términos del documento). Cuando el proceso ha finalizado, se muestra un resumen como el del video 2

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MeJ2mSIhtDY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


<center> Video 2: Preprocesamiento </center>


El proceso de limpieza debe verse como un proceso iterativo, ya que no se puede garantizar un procedimiento de limpieza idéntico al realizar una revisión exploratoria. Antes del proceso de limpieza, había 530143 términos (Original) en el corpus, sin embargo, el procedimiento redujo el número de términos únicos a 3268 (Final), reduciendo en gran medida las necesidades computacionales. La matriz DTM resultante se puede visualizar en el panel `Document Term Matrix Visualizations` tanto de forma tabular como de forma gráfica.

El botón `View Data` muestra algunas estadísticas básicas del corpus, term_freq que proporciona frecuencias de términos, doc_frec la cantidad de documentos en los que aparece cada término y la frecuencia inversa de documentos (idf), que miden la importancia de un término. También se muestran una serie de botones que permiten descargar la información  en formato CSV, EXCEL o PDF, imprimir el archivo, copiarlo al portapapeles, además, un botón para configurar el número de filas que se mostrarán en el resumen (Video 3).



Al hacer clic en el botón `View barplot`, se mostrará un gráfico de barras. El número de barras se puede configurar mediante el control deslizante que se muestra en el botón desplegable `Select number of term`. En la parte superior derecha del gráfico encontramos el botón `export`, el cual se utiliza para descargar el gráfico en diferentes formatos (png, jpeg, svg y pdf).



Al hacer clic en el botón `View worcloud`, se mostrará una nube de palabras. El número de palabras se puede configurar mediante el control deslizante que se muestra en el botón desplegable  `Select number of term` En la parte superior derecha del gráfico (botón `export`)permite descargar el gráfico en diferentes formatos (png, jpeg, svg y pdf).

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/sKKwNvwoX68" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


<center> Video 3: Visualización de la matriz DTM </center>


## Inferencia
Una vez obtenida la matriz DTM, el siguiente paso es determinar el número óptimo de tópicos.
Tenga en cuenta que encontrar el número de tópicos es un procedimiento computacional intensivo, el procedimiento puede llevar tiempo considerable (desde unos pocos minutos hasta incluso un par de días) dependiendo del tamaño del DTM, el número de modelos (número de tópicos a evaluar), número de iteraciones y el número de núcleos de su computadora ( LDAShiny funciona con detectCores () - 1 número de núcleos en su computadora).

En el video 4 se muestra las opciones de configuración para cada una de las métricas utilizadas en el calculó el número de temas, y los resultados gráficos de cada uno en el derecho.

El tiempo de procesamiento para cada una de las cuatro métricas fue 10408, 1987, 6018 y 1614 segundos para "coherence", "4-metrics", "perplexity" y "Harmonic mean", respectivamente. En cuanto al número de tópico, las métricas grifths2004 (Grifths y Steyvers 2004), CaoJuan2009 (Cao, Xia, Li, Zhang y Tang 2009), Arun2010 ((Arun, Suresh, Madhavan y Murthy 2010)), Deveaud2014 (Deveaud, SanJuan y Bellot 2014), ""perplexity" y "Harmonic mean" muestran que el número de temas adecuados está entre 35 y 50, mientras  "coherence" muestra 24 Tópicos. Por lo tanto, en este caso se prefirió el menor número de temas, ya que la intención es proporcionar una descripción general.


<center>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MxugkjidMrk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<center> Video 4: Inference </center>




## Modelado de temas

Una vez que se ha tomado una decisión sobre el número de temas, se ajusta el modelo LDA. La entrada es la matriz de términos del documento. Los parámetros de inferencia pueden usarse como una guía, sin embargo, algunos de ellos pueden cambiarse, como por ejemplo el número de iteraciones que puede ser mayor, y podríamos usar la recomendación de Grifths y Steyvers (2004) de usar un valor de 50/k para el parámetro  \alpha. En nuestro caso usamos 1000 iteraciones, 100 burnin y el valor  \alpha 2.08 como parámetros de entrada (Video 5).

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FQ84TBGn2XY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<center> Video 5: Modelo LDA </center>


La salida del modelo contiene varios objetos, la DTM es reducida a dos matrices. Una de ellas la matriz, theta ($$\theta$$), tiene filas que representan una distribución de tópicos en los documentos $$P\left ( topic_{k}\mid document_{d} \right )$$, la otra, phi ($$\phi$$), tiene filas que representan una distribución de palabras sobre tópicos $$P\left ( token_{v}\mid topic_{k} \right )$$. 

Estas matrices se pueden ver y descargar en el menú `Download tabular results`. También en este menú podemos ver y configurar un resumen del modelo en la pestaña he tab `Summary LDA` tres sliders se mostrarán en la parte superior los cuales permiten la configuración del resumen : `Select number of labels`, `Select number top terms`, and `Select assignments`.

`LDAShiny` proporciona un etiquetado de tópicos (el mismo que usa el paquete texrmineR) basado en un algoritmo de etiquetado  basado en n-gramamas, sin embargo, estos algoritmos tienen una capacidad limitada, pero pueden servir como guía.



En el menú `Download tabular results` haciendo clic en `Allocation button`, se mostrará una tabla donde el usuario puede observar los documentos que pueden ser organizados por tópicos. Gracias al control deslizante ubicado en la parte superior podemos elegir la cantidad de documentos por tema a mostrar.


Con el propósito de facilitar la caracterización de los tópicos en cuanto a su tendencia, LDAShiny `LDAShiny` pendientes de regresión simple para cada tópico, donde el año es la variable dependiente y la proporcion de los tópicos en el año correspondiente  la variable respuesta (Grifths  and Steyvers 2004 ).

<center>
$$\theta _{k}^{y}=\frac{\sum_{m\epsilon y}\theta _{mk}}{n^y}$$
</center>

Donde $m\epsilon y$  representa los artículos publicados en un año determinado y $$\theta_{mk}$$ la proporción del k-esímo tópico y  $$n^y$$ el número total de artículos publicados enel año *y* \citet{xiong2019analyzing}.Se considera que los tópicos cuyas pendientes de regresión son significativas (a un cierto nivel de significancia) tienen una tendencia creciente si son positivas y con tendencia decreciente en caso contrario. Si las pendientes no son significativas, se se asume que la tendencia del tópico es fluctuante.

al hacer clic en el boton `trend`aparece una tabla que muestra los resultados de una regresión lineal simple (intersección, pendiente, estadística de prueba, error estándar y valor p).

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5T5UoJVzcfg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<center> Video 6: Descarga de resultados tabulares </center>


Además de las opciones tabulares mencionadas, en el menú `Download graphic results` podemos encontrar tres botones `trend`, `View worcloud by topic`, y `heat_map`


Al hacer clic en `trend`, se mostrará un gráfico de líneas (una línea para cada tema) donde se pueden visualizar las tendencias temporales. El gráfico es interactivo, al hacer clic en las líneas, se eliminarán o mostrarán según lo decida el usuario.


Al hacer clic en `View worcloud by topic` se mostrará una nube de palabras. En el botón desplegable se puede seleccionar el tópico del cual queremos generar la nube de palabras, además, en el control deslizante se puede seleccionar la cantidad de palabras a mostrar .

Al hacer clic en `heatmap`, se mostrará un mapa de calor. Los años se muestran en el eje x, el eje y muestra los temas y la variación de color representa las probabilidades (Figura 19).

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5G7mTUtgHB4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<center> Video 7: Descarga de opciones gráficas </center>


# References

Arun R, Suresh V, Madhavan CV, Murthy MN (2010). On finding the natural number of topics with latent dirichlet allocation: Some observations. *In Pacific-Asia conference on
knowledge discovery and data mining*, pp. 391-402. Springer.


Blei DM, Ng AY, Jordan MI (2003). Latent Dirichlet allocation. *Journal of machine Learning
research* , **3**(Jan), 993-1022.


Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2017.
Shiny: Web Application Framework for R. https://cran.r-project.org/web/packages/
shiny/


Cao J, Xia T, Li J, Zhang Y, Tang S (2009). A density-based method for adaptive LDA
model selection.*Neurocomputing*, **72**(7-9), 1775{1781.


Deveaud R, SanJuan E, Bellot P (2014). Accurate and efective latent concept modeling for
ad hoc information retrieval. *Document numerique*, **17**(1), 61{84.


Grifths TL, Steyvers M (2004). Finding scientific topics. *Proceedings of the National
academy of Sciences*, **101**(suppl 1), 5228-5235.

